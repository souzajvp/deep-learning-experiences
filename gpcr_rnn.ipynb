{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpcr_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/souzajvp/deep-learning-experiences/blob/master/gpcr_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIracK06tiYP"
      },
      "source": [
        "# GPCR class prediction using amino acid sequence data\n",
        "\n",
        "Much of this code is adapted from - https://github.com/bentrevett/pytorch-sentiment-analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4lVbY8HtmN0"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzoqW5omKiDu"
      },
      "source": [
        "url ='https://raw.githubusercontent.com/souzajvp/deep-learning-experiences/master/train_gpcr.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfNtunHjtopQ"
      },
      "source": [
        "train = pd.read_csv(url)\n",
        "\n",
        "train.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o3RgxVntvR5"
      },
      "source": [
        "tokenize = lambda x: [char for char in x]\n",
        "TEXT = data.Field(tokenize = tokenize) \n",
        "LABEL = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE3T8xDyty02"
      },
      "source": [
        "fields = [('Sequence', TEXT), ('label', LABEL)] # Note: the order has to be the\n",
        "# same as the order of columns in your dataset!\n",
        "\n",
        "train_data = data.TabularDataset(\n",
        "    path = \"train_gpcr.csv\",\n",
        "    format = \"CSV\",\n",
        "    fields = fields,\n",
        "    skip_header = True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFOfN3Ndt83K"
      },
      "source": [
        "import random\n",
        "SEED = 42\n",
        "\n",
        "train_data, test_data = train_data.split(random_state = random.seed(SEED), split_ratio = 0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byECAQIEyKMe"
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J84uDW74uEUS"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "LABEL.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrA05n3UuHZq"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    sort_key = lambda x: len(x.Sequence),\n",
        "    sort_within_batch = False)\n",
        "\n",
        "# The last two arguments are because of -\n",
        "# https://github.com/pytorch/text/issues/474\n",
        "# Also try simply `sort = False`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK_tPNwjvfIm"
      },
      "source": [
        "# checking if everything looks ok\n",
        "\n",
        "for batch in test_iterator:\n",
        "  Sequence = batch.Sequence\n",
        "  label = batch.label\n",
        "  print(label)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjLJ0ecQwOVN"
      },
      "source": [
        "# Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgX7h9LduOAy"
      },
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, hidden = self.rnn(embedded)\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #what we need -\n",
        "        #hidden = [batch size, hid dim]      \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azZnrhBLufJd"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 50 \n",
        "HIDDEN_DIM = 25 \n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = RNN(INPUT_DIM,\n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSUKCs7mutT-"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IVqnsRuu_QG"
      },
      "source": [
        "for epoch in range(20):    \n",
        "    for batch in train_iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sequence = batch.Sequence \n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "# this might take several minutes to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwUrRl_vY0fL"
      },
      "source": [
        "len(train_data), len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOcw5WyPwUJe"
      },
      "source": [
        "correct = 0\n",
        "total = len(test_data)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        sequence = batch.Sequence  \n",
        "        label = batch.label\n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct += torch.sum((rounded_preds == label)).item()\n",
        "\n",
        "correct/total * 100 # 34.4%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os1eJl-kxaGV"
      },
      "source": [
        "# Simple LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMuIiWFSxcV7"
      },
      "source": [
        "class simple_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        #what we need -\n",
        "        #hidden = [batch size, hid dim]      \n",
        "        return self.fc(hidden.squeeze(0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXRCFQp1xrkN"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 50 \n",
        "HIDDEN_DIM = 25 \n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "model = simple_LSTM(INPUT_DIM,\n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KL7r_yGxwLF"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLYuq0d-x0E6"
      },
      "source": [
        "for epoch in range(20):    \n",
        "    for batch in train_iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sequence = batch.Sequence \n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "# this might take several minutes to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnH-fXuwx3ie"
      },
      "source": [
        "correct = 0\n",
        "total = len(test_data)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        sequence = batch.Sequence  \n",
        "        label = batch.label\n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct += torch.sum((rounded_preds == label)).item()\n",
        "\n",
        "correct/total * 100 # 59.24%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXNcEPSSx6Xw"
      },
      "source": [
        "# Two-layer bi-directional LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KPvZDEwx9PK"
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                            hidden_dim,\n",
        "                            num_layers = n_layers, \n",
        "                            bidirectional = bidirectional)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "        \n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "        \n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        \n",
        "        #hidden = [num layers * num directions, batch size, hid dim]\n",
        "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
        "\n",
        "        return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDT383tUy7Oi"
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 50 \n",
        "HIDDEN_DIM = 25 \n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "model = LSTM(INPUT_DIM,\n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM,\n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3rh8F0EzK33"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr = 1e-2)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFkekYZtzMBq"
      },
      "source": [
        "for epoch in range(20):    \n",
        "    for batch in train_iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        sequence = batch.Sequence \n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        loss = criterion(predictions, batch.label)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Loss:\", loss)\n",
        "\n",
        "# this might take several minutes to run"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQbtJH__zS93"
      },
      "source": [
        "correct = 0\n",
        "total = len(test_data)\n",
        "    \n",
        "with torch.no_grad():\n",
        "    for batch in test_iterator:\n",
        "        sequence = batch.Sequence  \n",
        "        label = batch.label\n",
        "\n",
        "        predictions = model(sequence).squeeze(1)\n",
        "        \n",
        "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
        "        correct += torch.sum((rounded_preds == label)).item()\n",
        "\n",
        "correct/total * 100 # 97%"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfdNcB9h0H-5"
      },
      "source": [
        "#torch.save(model.state_dict(), 'gpcr_model_03_29_2021.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHGkiJzST9XB"
      },
      "source": [
        "# model.load_state_dict(torch.load('gpcr_model_03_28_2021.pt'))\n",
        "model.load_state_dict(torch.load('gpcr_model_03_29_2021.pt'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvrXY4DGc2nI"
      },
      "source": [
        "# Test model on individual input sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--dcY5C0Jeu6"
      },
      "source": [
        "def predict_gpcr(model, sequence):\n",
        "    tokenized = lambda x:[char for char in x]\n",
        "    tokenized = tokenized(sequence)\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    tensor = torch.LongTensor(indexed).to(device) \n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    prediction = torch.sigmoid(model(tensor))\n",
        "    return prediction.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3M1WuKgKPGA"
      },
      "source": [
        "at1r = \"MILNSSTEDGIKRIQDDCPKAGRHNYIFVMIPTLYSIIFVVGIFGNSLVVIVIYFYMKLKTVASVFLLNLALADLCFLLTLPLWAVYTAMEYRWPFGNYLCKIASASVSFNLYASVFLLTCLSIDRYLAIVHPMKSRLRRTMLVAKVTCIIIWLLAGLASLPAIIHRNVFFIENTNITVCAFHYESQNSTLPIGLGLTKNILGFLFPFLIILTSYTLIWKALKKAYEIQKNKPRNDDIFKIIMAIVLFFFFSWIPHQIFTFLDVLIQLGIIRDCRIADIVDTAMPITICIAYFNNCLNPLFYGFLGKKFKRYFLQLLKYIPPKAKSHSNLSTKMSTLSYRPSDNVSSSTKKPAPCFEVE\"\n",
        "nupr1 = \"MATFPPATSAPQQPPGPEDEDSSLDESDLYSLAHSYLGGGGRKGRTKREAAANTNRPSPGGHERKLVTKLQNSERKKRGARR\"\n",
        "\n",
        "predict_gpcr(model, at1r) \n",
        "predict_gpcr(model, nupr1) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk3tcCG0zv3J"
      },
      "source": [
        "# References\n",
        "\n",
        "Deep Learning review paper - https://www.nature.com/articles/nature14539  \n",
        "Deep Learning and Biological Sequence data review paper - https://pubmed.ncbi.nlm.nih.gov/28961695/  \n",
        "Understanding LSTM Networks - https://colah.github.io/posts/2015-08-Understanding-LSTMs/  \n",
        "The Unreasonable Effectiveness of Recurrent Neural Networks - http://karpathy.github.io/2015/05/21/rnn-effectiveness/  \n",
        "Positional SHAP for Interpretation of Deep Learning Models Trained from Biological Sequences - https://www.biorxiv.org/content/10.1101/2021.03.04.433939v1  \n"
      ]
    }
  ]
}